{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e86a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the solutions to the assignment questions:\n",
    "\n",
    "# ### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# R-squared (R²) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides an indication of the goodness of fit of a model.\n",
    "\n",
    "# **Calculation:**\n",
    "# \\[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n",
    "# where:\n",
    "# - \\( SS_{res} \\) = Sum of squares of residuals\n",
    "# - \\( SS_{tot} \\) = Total sum of squares\n",
    "\n",
    "# **Representation:**\n",
    "# - R² = 0 means that the model explains none of the variability of the response data around its mean.\n",
    "# - R² = 1 means that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "# ### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. It incorporates the model complexity (number of predictors) into the calculation and is more appropriate when comparing models with different numbers of predictors.\n",
    "\n",
    "# **Calculation:**\n",
    "# \\[ \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1-R^2)(n-1)}{n-k-1} \\right) \\]\n",
    "# where:\n",
    "# - \\( n \\) = Number of observations\n",
    "# - \\( k \\) = Number of predictors\n",
    "\n",
    "# **Difference:**\n",
    "# - Regular R-squared can be artificially high due to the inclusion of more predictors.\n",
    "# - Adjusted R-squared adjusts for the number of predictors and does not automatically increase with the addition of more predictors.\n",
    "\n",
    "# ### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# Adjusted R-squared is more appropriate to use when comparing the goodness of fit of regression models that have a different number of predictors. It provides a more accurate measure of model performance by accounting for model complexity.\n",
    "\n",
    "# ### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# - **RMSE (Root Mean Squared Error):** Measures the square root of the average of squared differences between predicted and actual values.\n",
    "#   \\[ RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2} \\]\n",
    "\n",
    "# - **MSE (Mean Squared Error):** Measures the average of the squared differences between predicted and actual values.\n",
    "#   \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\]\n",
    "\n",
    "# - **MAE (Mean Absolute Error):** Measures the average of the absolute differences between predicted and actual values.\n",
    "#   \\[ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}| \\]\n",
    "\n",
    "# **Representation:**\n",
    "# - These metrics are used to evaluate the accuracy of a regression model. Lower values indicate better model performance.\n",
    "\n",
    "# ### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# **RMSE:**\n",
    "# - Advantages: Penalizes larger errors more than smaller errors, providing a useful metric for models where large errors are particularly undesirable.\n",
    "# - Disadvantages: Sensitive to outliers due to squaring errors.\n",
    "\n",
    "# **MSE:**\n",
    "# - Advantages: Simple to calculate and differentiable, making it useful for optimization algorithms.\n",
    "# - Disadvantages: Similar to RMSE, it is sensitive to outliers.\n",
    "\n",
    "# **MAE:**\n",
    "# - Advantages: Less sensitive to outliers compared to RMSE and MSE, providing a more robust measure in the presence of outliers.\n",
    "# - Disadvantages: Does not penalize large errors as strongly as RMSE.\n",
    "\n",
    "# ### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# **Lasso Regularization (L1):**\n",
    "# - Adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "#   \\[ \\text{Lasso Penalty} = \\lambda \\sum_{j=1}^{p} | \\beta_j | \\]\n",
    "# - Tends to produce sparse models with few coefficients, effectively performing feature selection.\n",
    "\n",
    "# **Ridge Regularization (L2):**\n",
    "# - Adds a penalty equal to the square of the magnitude of coefficients to the loss function.\n",
    "#   \\[ \\text{Ridge Penalty} = \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "# - Shrinks coefficients but does not eliminate them, maintaining all predictors in the model.\n",
    "\n",
    "# **Appropriate Usage:**\n",
    "# - Use Lasso when feature selection is desired or when dealing with high-dimensional data.\n",
    "# - Use Ridge when multicollinearity is present, and all predictors are thought to be relevant.\n",
    "\n",
    "# ### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# Regularized linear models add a penalty to the loss function based on the magnitude of the coefficients, which discourages the model from fitting the noise in the training data, thus preventing overfitting.\n",
    "\n",
    "# **Example:**\n",
    "# In a dataset with many predictors, a simple linear regression might overfit by creating a complex model that captures the noise. By applying Ridge or Lasso regularization, the model complexity is controlled, resulting in a simpler model that generalizes better to unseen data.\n",
    "\n",
    "# ### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# **Limitations:**\n",
    "# - Regularized models assume that there is a linear relationship between predictors and the response, which may not always be the case.\n",
    "# - Lasso can arbitrarily eliminate important predictors if they are correlated with others.\n",
    "# - Regularization introduces bias into the estimates, which might lead to underfitting if the regularization parameter is too high.\n",
    "\n",
    "# **Why They May Not Always Be Best:**\n",
    "# - For non-linear relationships, non-linear models or transformations might be more appropriate.\n",
    "# - In scenarios where all predictors are important, the feature selection aspect of Lasso may be undesirable.\n",
    "\n",
    "# ### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# Choosing the better performer depends on the context and the importance of large errors:\n",
    "# - If large errors are particularly undesirable, Model A (with RMSE) might be preferred because RMSE penalizes larger errors more heavily.\n",
    "# - If robustness to outliers is more important, Model B (with MAE) might be preferred due to its less sensitivity to outliers.\n",
    "\n",
    "# **Limitations:**\n",
    "# - RMSE might be overly influenced by outliers.\n",
    "# - MAE might not sufficiently penalize large errors in critical applications.\n",
    "\n",
    "# ### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "# **Answer:**\n",
    "\n",
    "# Choosing between Ridge and Lasso depends on the context:\n",
    "# - If feature selection is important, Model B (Lasso) might be preferred as it can produce a sparser model.\n",
    "# - If dealing with multicollinearity and all predictors are relevant, Model A (Ridge) might be preferred as it shrinks coefficients without eliminating them.\n",
    "\n",
    "# **Trade-offs:**\n",
    "# - Lasso might eliminate relevant features if they are correlated with others.\n",
    "# - Ridge does not perform feature selection, which might be less interpretable in high-dimensional settings.\n",
    "\n",
    "# **Limitations:**\n",
    "# - The choice of the regularization parameter (\\(\\lambda\\)) can significantly affect the model performance and needs to be carefully tuned through cross-validation or other methods.\n",
    "\n",
    "# To complete the assignment, you should create a Jupyter notebook with these answers, including code examples and explanations where appropriate. Once completed, upload the notebook to a public GitHub repository and share the link through your dashboard as instructed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
